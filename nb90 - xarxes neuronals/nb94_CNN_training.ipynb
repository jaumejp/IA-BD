{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5f8724-c96d-42df-88e1-3b6256e7d1bb",
   "metadata": {},
   "source": [
    "### CNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "314510f3-5d79-41ff-b5ef-477bdea3ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9be390b-fa6a-420c-93d6-6e439152970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import precision_recall_fscore_support as prf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84115ec8-5b5d-4c34-b7c1-55138b2304b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ExifTags, ImageOps, ImageDraw\n",
    "from src import bbox2tlbr, sqrbbox, compute_IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68a7a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e881a5",
   "metadata": {},
   "source": [
    "### 1. Dataset & dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f01eeb",
   "metadata": {},
   "source": [
    "- Dataset y dataloader &rarr; fer treballar la cpu i gpu en paral·let\n",
    "- Avui veiem com entrenar la xarxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23fc51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e800c7c",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b2eaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "_imgRoot = '../mosquits/phase2_test/test/final/'\n",
    "\n",
    "# El dataset l'hauríem de complimentar amb algunes imatges sense mosquit per tenir '??'\n",
    "# _classes = ['aegypti', 'albopictus', 'anopheles', 'culex', 'culiseta', 'japonicus-koreicus', '??']\n",
    "_classes = ['aegypti', 'albopictus', 'anopheles', 'culex', 'culiseta', 'japonicus-koreicus']\n",
    "\n",
    "# El que comentava al final del anterior notebook\n",
    "_imgSize = 512 # pretrained at 384\n",
    "\n",
    "_imgNorm = ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "\n",
    "# control image max-size \n",
    "Image.MAX_IMAGE_PIXELS = 201326592\n",
    "Image.warnings.simplefilter('error', Image.DecompressionBombWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32b893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class maDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csvDataFile):\n",
    "        \n",
    "        self.df = pd.read_csv(csvDataFile)\n",
    "        self.transform = transforms.Compose([\n",
    "                transforms.Resize((_imgSize, _imgSize)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(_imgNorm[0], _imgNorm[1])\n",
    "            ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        # open image file\n",
    "        row = self.df.iloc[idx]\n",
    "        pilImg = Image.open('%s/%s' %(_imgRoot, row.img_fName))\n",
    "        # crop image\n",
    "        bbox = sqrbbox([(row.bbx_xtl, row.bbx_ytl), (row.bbx_xbr, row.bbx_ybr)], pilImg.size)\n",
    "        pilImg = pilImg.crop(bbox)\n",
    "        # transform to torch tensor image\n",
    "        torchImg = self.transform(pilImg)\n",
    "\n",
    "        return {'img_fName': row.img_fName, 'image' : torchImg, 'label': [_classes.index(row.class_label)]}\n",
    "        # row class_label no són etiquetes alphanumeriques, ho convertim a numeric dins del index\n",
    "        # Algo que ho converteixi a integers\n",
    "        # si és la etiqueta 3, la neurona que s'ha d'activar més es la 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161e86e",
   "metadata": {},
   "source": [
    "- Quan volem fer una subclasse de la dataset torch, ha de tenir dos metodes: \n",
    "    - len \n",
    "    - getitems\n",
    "    - definir els transforms\n",
    "- Flexible size: \n",
    "    - les podem entrenar a 512 i fer inferencia a una altre\n",
    "- Input size d'una determinada xarxa, en molts casos el que pasa es que aquesta mida és flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c45db0a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2763"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate our custom dataset\n",
    "csvDataFile = '../mosquits/phase2_test.csv'\n",
    "_maDataset = maDataset(csvDataFile)\n",
    "_maDataset.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346b663",
   "metadata": {},
   "source": [
    "- Fem com si el dataset de entrenament serà el de test, només per veure com va. \n",
    "    - Perquè la màquina es petita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a06dda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the dataloader\n",
    "batch_size, num_workers = 4, 8\n",
    "_dataLoader = DataLoader(_maDataset, batch_size = batch_size, shuffle = True, num_workers = num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5e719",
   "metadata": {},
   "source": [
    "- shuffle true, ens interesa que a cada epoch tenir un shuffle diferent. tenir batch diferents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b008a3fd",
   "metadata": {},
   "source": [
    "### 2. Backbone\n",
    "- espina dorsal, es fa servir per referisa a la arquitectura del model\n",
    "- el nostre backbone es una efficientnet\n",
    "- dins el concepte backbone es separa el classificador final. \n",
    "- backbone es fins el convolutional head i després s'afegeix el classificador\n",
    "- la fc -> fully conected layer, no està inclosa en el backbone. \n",
    "- cada persona se'l ajusta com vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd57ac40",
   "metadata": {},
   "outputs": [],
   "source": [
    "_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ad9ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dc9108",
   "metadata": {},
   "source": [
    "- abans toarch load i carregavem model .pth, model.\n",
    "- aquí partim de la arquitectura pre entreada de la gent (com la van entrenar els seus autors).\n",
    "- Això ho fem amb el timm &rarr; hi han moltes arquitectures guardades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a8363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "model = timm.create_model(\n",
    "        'tf_efficientnetv2_s',\n",
    "    \n",
    "        # Si volem partir del pre entrenament dels autors o no\n",
    "        pretrained = True,\n",
    "        num_classes = len(_classes),\n",
    "        global_pool = 'avg'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0050ef",
   "metadata": {},
   "source": [
    "- pensar-ho com si la extracció d'atributs que ha fet aquesta gent ens serveix o no\n",
    "- les fotos que han fet srvir són molt igual totes (taules, cadires, gats, etc)\n",
    "- tot el procesament d'atributs que la xarxa ha aprés, podem pensar que es extrapolable a objectes del nostre món. \n",
    "- si en comptes de analitzar fotos de imatges del nostre mon, analitzem fotos de cel·lules per detectar càncer\n",
    "- pot no tenir res a veure amb objectes del mon\n",
    "- es posible que si partim de una pre enetrenada costi fer un fine tinning\n",
    "- un escaner del cervell\n",
    "- asumim pels mosquits va bé\n",
    "- hi ha un altre factor, si hi ha una xarxa de 0, pesos aleatoris, perque volem una xarxa molt específica de mosquits, amb 2000 fotos ni 10000 en fem prou, necesitem una altre ordre de magnitud, fer fer convergir la xarxa cap a un bon resultat. Si les tenim, la xarxa serà molt més eificents. \n",
    "- num_classes: el efficentnet té moltes clases.\n",
    "    - això el que farà en termes de arquitectura, canvia les neurones de sortida, en tenia 10000 perque hi havia 10000 classes, nosaltres li indiquem les que volem. En volem 6. No 7 perquè en el nostre set d'entrenament no li passarem fotos sense mosquit.\n",
    "- Els pesos que es toquen en el fine tuning només es fa en aquesta ultima capa i pesos, els altres ja ens els creiem.\n",
    "- els altre pesos no els toquem.\n",
    "- Lu correcte sería posar imatges amb background i etiqueta ?? i posar-ho al dataset i afegir aquesta neurona 7 per les que no sap identificar.\n",
    "- global_pool: això ve a dir com farem l'agregat dels arros de la funció d'erro en el batch.\n",
    "    - podem fer suma, promig, el maxim\n",
    "    - com s'agreguene els errors en la funció de batch, aquí fem un promig. \n",
    "    - vas provant \n",
    "    - provem avg, i després provem max o sum\n",
    "    - cada canvi són 3 dies d'entrenament, cada canvi es a nivell de temps i energia.\n",
    "- quan parlavem de pooling, les capes qeu fan el kernel de dimensió 1, és el mateix concepte però amb un altre significat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "977c110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generem el model amb aquesta arquitectura i el classificador i el passem a la gpu:\n",
    "model = model.to(_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c6969",
   "metadata": {},
   "source": [
    "### 3. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad00c0dd",
   "metadata": {},
   "source": [
    "#### loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36fe541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53a63de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6343e721",
   "metadata": {},
   "source": [
    "- Quan fem train, hem de contrastar l'output per actualitzar els pesos. \n",
    "- Funció de loss: \n",
    "    - torch nn : hi ha varies funcions de loss. \n",
    "    - error mean: $|y_i+y\\hat{_i}|$, mean squared error, el mateix al quadrat en comptes de error abs. \n",
    "    - això va bé per models de regressió. \n",
    "    - en cas de classificació aquestes mesures no acaben d'anar bé. \n",
    "    - lu habitual és: \n",
    "        - binnari cross entropy (BCE) -> només 2 categoríes\n",
    "        - cross entropy (CE) -> baries categories +2\n",
    "        - hi han més.\n",
    "        - si tenim una distribució de probabilitat, tenim que la nostre xarxa ens diu: \n",
    "        - p = [p1, p2, ...px] -> cada p es una probabilitat, la suma de totes es 1\n",
    "        - la entropia de una distribució de probabilitat es una mesura de cuant incertasa hi ha en aquesta distribució. \n",
    "        - es defineix com: en comptes de la suma\n",
    "        - sumatori de $p_i$ · log($p_i$)\n",
    "        - si no hi ha cap incertesa, sabem el que passarà, es perque n'hi ha una classe que es 1. \n",
    "        - 1·log(1) + 0·log(0) ... totes 0\n",
    "        - = 0 -> la incertesa és 0\n",
    "        - si es una distribució unforme, tots iguals, tots poden soritr amb la mateixa probabilitat = maxima incertesa\n",
    "        - p = [1/k, 1/k, 1/k ...]\n",
    "        - 1/k·log(1/k) + 1/k·log(1/k) ... tants cops com k = k · 1/K · log(1/k) = 1 · log(1/k) = és la entropia\n",
    "        - H(P) = CE/BCE = sumatori de tata el que hi ha a dalt.\n",
    "        - es una mesura que va bé"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b6341e",
   "metadata": {},
   "source": [
    "#### optimizer\n",
    "- agafa l'error, calculat per la funció de cost. en el context de nn és fa servir la paraula loss\n",
    "- si l'error és més gran, s'ajustaran més\n",
    "- en cada epoch s'ha d'anar ajustant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8d7a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8285ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "lRate = 0.0001\n",
    "optimizer = optim.Adam(model.parameters(), lr = lRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25cb35ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.0001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer\n",
    "# Optimizer té els parametres del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812304f3",
   "metadata": {},
   "source": [
    "- modul optim hi han diferents tipus d'optimitzadors. Estocastic gradient descent, adam, etc\n",
    "- diferents conceptes matemàtics. \n",
    "- adaptive optimizer algo (adam) va molt bé. \n",
    "    - perque tots els parametres de la xarxa es vagin optimitzant a la vegada. \n",
    "    - abans que es desenvoupes l'adam, hi havia casos que una part de la nn s'actualitzava bé i una altre part no. aquest ho balançeja bé. \n",
    "- ---\n",
    "- a adam li passem, tots els parametres del model, quins parametres ha de optimitzar (funció `.parameters()`)\n",
    "    - no torna un numero de parametres, sino una estructura, quins són i com estan relalcionats\n",
    "- li passem el learning rate. \n",
    "    - Té a veure amb quina mesura actualitzem els pesos en funció de la magnitud dels errors. \n",
    "    - learning rate 0.5, si tenim error de x, actualitzem l'error com si hagues sigut de la meitat. perque l'error ve d'uns exemples en concret. \n",
    "    - tant percent de credibilitat que li donem als errors que hem generat. Ens creiem una deu milèsima de l'error.\n",
    "- funciona millor primer learning rate petit i després gran. perque al principi no saps, si li fas creure a la xarxa que no sap res, si està classificant cara o creu, si fas un gran al principi, fas actualitzacions grans molt ràpides, si allò ha converit i canvies ja no s'aren res\n",
    "- al principi fas petit i després gran. pero la majoria agafa scheduler descendets però per en Joan es un error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d17ba",
   "metadata": {},
   "source": [
    "#### scheduler\n",
    "- canvia el learning rate en base una funció"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cadd21fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.005\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a18c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.optim.lr_scheduler.StepLR at 0x7ff05f74b190>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593592c",
   "metadata": {},
   "source": [
    "- li passem el optimitzador i el step, a cada pas el modificarà. \n",
    "- el parametre important és gamma.\n",
    "- el que volem es que el learning rate vagi augmentant poc a poc. es la manera de seguir apretant la xarxa.\n",
    "- Podem fer un plot del learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d88551",
   "metadata": {},
   "source": [
    "### 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a00df380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83ed2c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++ epoch   0, 260.9493s Train- Loss: 0.2328 Acc: 0.9160\n",
      "+++ epoch   1, 262.0361s Train- Loss: 0.3662 Acc: 1.8683\n",
      "+++ epoch   2, 262.1921s Train- Loss: 0.4917 Acc: 2.8274\n",
      "+++ batch_inference() KeyboardInterrupt, \n",
      "CPU times: user 15min 47s, sys: 5.09 s, total: 15min 52s\n",
      "Wall time: 15min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train\n",
    "try:\n",
    "    \n",
    "    # Posar el model en mode train\n",
    "    model.train()\n",
    "    \n",
    "    # Al de abans no calia fer el with ... posaríem el False i ja està. \n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    # mostrem totes les imatges dues vegades prque sino tarda molt\n",
    "    # hi hauran casos que la xarxa apendra más ràpid, més lent, etc, ensayo i error.\n",
    "    max_epochs = 4\n",
    "    \n",
    "    # loop per totes les epoch\n",
    "    for epoch in range(max_epochs):\n",
    "        # Variables que anirem veient el loss que tenim en cada epoch per anar fent control (no funciona)\n",
    "        train_loss, train_match = .0, .0\n",
    "        \n",
    "        # Mirar el que tarda la epoc\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Iterem per el data loader, aquest ens anirà donant batch de imatges fins que ens les donarà totes.\n",
    "        # Iterar per el dataset a base de batch\n",
    "        for batch in _dataLoader:\n",
    "            \n",
    "            # per cada batch\n",
    "            \n",
    "            # +++ forward pass\n",
    "            optimizer.zero_grad() # S'han de posar a 0\n",
    "            inputs = batch['image'].to(_device) # del lot agafem les imatges, passem a la gpu\n",
    "            output = model(inputs) # Fem inferència (però amb uns pesos aleatoris)\n",
    "            \n",
    "            # +++ loss\n",
    "            labels = torch.cat(tuple(batch['label']), dim = 0).to(_device) # Agafem les etiquetes, que hem posat en el dataset, quan hem definit la classe, s'ha de fer amb tensors, però no està amb la dimensió que toca\n",
    "            batch_loss = loss_function(output, labels) # les etiquetes són numeros no strings i representen les neurones d'activació (si es label 3, la més activada ha de ser la 3)\n",
    "            \n",
    "            \n",
    "            # +++ backpropagation\n",
    "            batch_loss.backward() # La funció de loss es un objecte que té un metode\n",
    "            optimizer.step() # step fa el update, el step ja té el learning rate\n",
    "            \n",
    "            # El optimizer té els parametres del model i és així com sap on s'han d'anar a actualitzar\n",
    "            \n",
    "            # +++ evaluation\n",
    "            \n",
    "            # Ho convertim a predicccions i fem el hard prediction\n",
    "            # _ es per algo que no volem, ho posa per veure que vagi baixant i així pot parar l'entrenament si no va bé\n",
    "            _, preds = torch.max(torch.nn.functional.softmax(output, dim = 1), dim = 1)\n",
    "            \n",
    "            # com que fem un average del loss, el multipliquem per el shape per tenir un loss més real\n",
    "            # Interesa com va evolucionant, ens importa la tendència.\n",
    "            # inputs és el tamany del batch, si posem la mida del batch no perque es un diccionari.\n",
    "            train_loss += batch_loss.data * inputs.shape[0]\n",
    "            \n",
    "            # Accuracy, summar quants cops les nostres prediccions són correctes\n",
    "            train_match += torch.sum(preds.data == labels.data)\n",
    "\n",
    "        print('+++ epoch {:3d}, {:6.4f}s Train- Loss: {:.4f} Acc: {:.4f}\\n'.format(epoch, (time.time() -start_time), train_loss.item() /_maDataset.__len__(), train_match.item() /_maDataset.__len__()), end = '')\n",
    "        \n",
    "except BaseException as err:    \n",
    "    print(f\"+++ batch_inference() {type(err).__name__}, {err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796c455c",
   "metadata": {},
   "source": [
    "- si mirem tenim 4/8 podríem augmentar el batch size\n",
    "- però 8 min per fer 2 epoch\n",
    "- ---\n",
    "- aquest script amb tensorflow són dues línies de codi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c1feb",
   "metadata": {},
   "source": [
    "### 5. Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "708b4442-033c-4534-ad81-e48e9cff03a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_pred \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df_pred\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda3/envs/entorn/lib/python3.10/site-packages/pandas/core/frame.py:830\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, abc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__array__\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    829\u001b[0m         \u001b[38;5;66;03m# GH#44616 big perf improvement for e.g. pytorch tensor\u001b[39;00m\n\u001b[0;32m--> 830\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    831\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    832\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/entorn/lib/python3.10/site-packages/torch/_tensor.py:1062\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1062\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "df_pred = pd.DataFrame(preds)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74491a20-505c-4f94-9659-9e6ae0a8dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.merge(_maDataset.df, df_pred, how = 'inner', on = 'img_fName')\n",
    "df_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97572b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.groupby('class_label').pred_label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa35fe-e1b9-477c-8ef9-5f62cd705e48",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546d324e-6e39-4b21-96ca-1aef08264a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(1, 3, figsize = (15, 7), sharey = True)\n",
    "for i, norm in enumerate([None, 'true', 'pred']):\n",
    "    ConfusionMatrixDisplay.from_predictions(\n",
    "        df_.class_label,\n",
    "        df_.pred_label,\n",
    "        normalize = norm,\n",
    "        ax = axs[i],\n",
    "        display_labels = ['??', 'aeg', 'alb', 'ano', 'clx', 'cul', 'j/k'],\n",
    "        cmap = 'GnBu',\n",
    "        colorbar = None\n",
    "    )\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63202458-c4df-4b3a-805f-36fe64359755",
   "metadata": {},
   "outputs": [],
   "source": [
    "avrgs = ['macro', 'micro', 'weighted']\n",
    "pd.DataFrame([prf(df_.class_label, df_.pred_label, average = mode, zero_division = 0)[:3] for mode in avrgs], columns = ['precision', 'recall', 'f-score'], index = avrgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b6cd2-f2ba-4587-96e7-6261bb4dc34f",
   "metadata": {},
   "source": [
    "#### check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c40f3-002c-41f4-a0b9-74b5f719a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d0b5e8-44a8-4ba4-83af-85eafa77a17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1\n",
    "row = df_.iloc[i]\n",
    "pilImg = Image.open('%s/%s' %(_imgRoot, row.img_fName))\n",
    "imgdrw = ImageDraw.Draw(pilImg)\n",
    "imgdrw.rectangle([(row.bbx_xtl, row.bbx_ytl), (row.bbx_xbr, row.bbx_ybr)], outline = 'blue', width = 2)\n",
    "plt.imshow(pilImg)\n",
    "plt.axis('off');\n",
    "print('+++%3d %s - %s / %s' %(i, row.img_fName, row.class_label, row.pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d2d31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk = df_[df_.pred_label == '??']\n",
    "len(chk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "j += 1\n",
    "row = chk.iloc[j]\n",
    "pilImg = Image.open('%s/%s' %(_imgRoot, row.img_fName))\n",
    "imgdrw = ImageDraw.Draw(pilImg)\n",
    "imgdrw.rectangle([(row.bbx_xtl, row.bbx_ytl), (row.bbx_xbr, row.bbx_ybr)], outline = 'blue', width = 8)\n",
    "plt.imshow(pilImg)\n",
    "plt.axis('off');\n",
    "print('+++%3d - %s / %s' %(i, row.class_label, row.pred_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
