{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "025449cf",
   "metadata": {},
   "source": [
    "### Modul de __ funciona amb: \n",
    "- `conda install gensim -c conda-forge`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2903a820",
   "metadata": {},
   "source": [
    "#### Comentaris de classe:\n",
    "- words embedings part fonamental de models per prosesament de llenguatge LLM (chat gpt) large languaje models. \n",
    "- input en forma de text i el model respon. en base: \n",
    "    - generar la paraula que amb més probabilitat seguiria la última que hem dit. \n",
    "- ho fonamenta el word embeding. És: \n",
    "    - tenim un espai de dades i aquest espai de dades i el volem reduir amb un espai inferior.\n",
    "    - un text pot tenir milers de paraules. si considerem l'espai dels textos, no ho podem determinar. \n",
    "    - fem un embedding, aquest espai el representem d'una altre manera (el fem encaixar dintte un espai que ens inventem i definim nosaltres la dimensió). \n",
    "- Ho resoldrem amb xarxes neuronals. Si tenim capes i neurones, tenim el vector, tenim el convolutional head. De 12 millons de pixels, ens inventem una manera de representar-la en un vector de 512 unitats. \n",
    "- Imatge de X pixels a través de la xarxa la convertim en un vector. \n",
    "- Dataset dels pisos, tenim una taula amb 10/15 columnes que defineixen els atributs. Imatges de pisos que pasen a un vector que sería la row del detaset. Ho hem de fer amb textos\n",
    "- Agafem textos, esl tokenitzem, els fem entar a la NN i en surt un vector, que es una descripció estructurada del text que hem entrat. \n",
    "- Qualsevol text es converteix en un xurro de numeros. aixo volem que ens serveixi per comparar uns textos amb els altres. \n",
    "- no només fer la conversió, sino que també hem de mantenir la estructura de dades que hi havia (forma de dades) els que s'assemblen entre si. que l'embeding preservi les similituds. Ajustar els pesos perquè tinguin més error o menys segons lo bé que prediguin les dades d'entrada. \n",
    "- Les semblançes són en termes de la semantica del text.\n",
    "- Saber quines són més semblants, vector home - vector genere = vecteor dona. \n",
    "- ---\n",
    "- el corpus de text, són les paraules d'entrenament. \n",
    "- si fem una matriu /dataset a les columnes documents i a les files paraules\n",
    "- posem un 0/1 si surt la paraula a cada document\n",
    "- matriu de 0 i 1 que diu si una determinada paraula està en un determinat document o no. \n",
    "- si agafem una fila (una paraula), que estaba a a \"X\" documents. \n",
    "- i n'agafem una altre (estàra en uns altres documents). \n",
    "- són dos paraules que entre elles existeix una coorelacio dels docuemnts que apareix. \n",
    "- si apareix en un també apareix en un altre. \n",
    "- document és qualsevol cosa que tingui text (paragraf, llibre, etc)\n",
    "- si busquem la corelacio.\n",
    "- per tots els documents (columnes). \n",
    "- la corelació entre la paraula (fila) 250 i la 3. \n",
    "- la podem mesurar com pel document 1 1·0, pel document 2 0·1, pel document x 1·1 ··· = 2\n",
    "- com més vegades es repeteix dos paraules en dos documents diferents més gran és la corelació\n",
    "- si sempre que està en un document no està en l'altre serpa 1·0 o 0·1, la seva corelació sempre serà 0. \n",
    "- també podem parlar de coorelació entre documents. Si apareixen les mateixes paraules en els mateixos documents tindrem més 1·1 i voldrà dir que són més semblants. \n",
    "- paraules molt coorelacioneades vol dri que estaran relacionades semanticament (cotxe i roda) o perque formen part d'un domini que són habituals (delanter i defensa). \n",
    "- si hi han dos paraules que acostumen a anar una després de l'altre, sempre que apreixi en un document, quan apareixi en l'altre document, la següent també hi serà. això es per mesurar com de possible es que una paraula vingui a continuació d'una altre. \n",
    "- només es algo que pasaria: les paraules (jo penso que)\n",
    "- molts documents amb jo, penso, que totes amb 1\n",
    "- no estem parlant de ordre encara (position embeding). no que una darrere l'altre sino que apareixaren juntes. \n",
    "- ---\n",
    "- algebra que a partir d'aquesta idea que fa que podem entrenar aquestes xarxes neuronals per fer el encoding (encoding i embeding) és el mateix però embeding especificament a transformar o reduir l'espai de dades. . \n",
    "- un text és un punt. o paraula és un punt. podem veure com de similars són entre si. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f84d3",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "Word2Vec Google 2013 C implementation\n",
    "\n",
    "\"show me your friends and I'll tell you who you are\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d383d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec9fb9",
   "metadata": {},
   "source": [
    "#### Load Simpsons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8625b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_682923/663137918.py:1: DtypeWarning: Columns (4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('./simpson_data/simpsons_script_lines.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./simpson_data/simpsons_script_lines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210718f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_character_text</th>\n",
       "      <th>spoken_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>No, actually, it was a little of both. Sometim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>Where's Mr. Bergstrom?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Miss Hoover</td>\n",
       "      <td>I don't know. Although I'd sure like to talk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lisa Simpson</td>\n",
       "      <td>That life is worth living.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Edna Krabappel-Flanders</td>\n",
       "      <td>The polls will be open from now until the end ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        raw_character_text                                       spoken_words\n",
       "0              Miss Hoover  No, actually, it was a little of both. Sometim...\n",
       "1             Lisa Simpson                             Where's Mr. Bergstrom?\n",
       "2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n",
       "3             Lisa Simpson                         That life is worth living.\n",
       "4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['raw_character_text', 'spoken_words']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def18a2",
   "metadata": {},
   "source": [
    "- make sure all text documents are type 'str'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e65b8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spoken_words'] = df.spoken_words.astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef40b6e",
   "metadata": {},
   "source": [
    "#### Data preprocessing (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8566b41",
   "metadata": {},
   "source": [
    "- classes per fer la tokenitzacio: `tokenize` és més a baix nivell i té una configuració que pot ser que algun dia haguem de tocar. \n",
    "- si el `simple_preprocess` -> spam/ham, treure caracters, treure accents, convertir minuscules (amb català, ens interessaría que més sigui diferent a mes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174d7eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f04e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [no, actually, it, was, little, of, both, some...\n",
       "1                               [where, mr, bergstrom]\n",
       "2    [don, know, although, sure, like, to, talk, to...\n",
       "Name: spoken_words, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_data = df.spoken_words.apply(simple_preprocess)\n",
    "preprocessed_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d2d5c3",
   "metadata": {},
   "source": [
    "- La tokenització de LLM va més enllà i codifica per trosos per blocs, cotxes seríen dos blocs (cotxe) i (s). Es pot fer a molts nivells de finura. \n",
    "- Conservar els interrogants del final, etc\n",
    "- No hi ha necessitat de baixar al nivell de cada token sigui un caràcter. \n",
    "- si ja fa alucinació amb paraules, si ho fes amb caracters, faría paraules noves."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e859dba",
   "metadata": {},
   "source": [
    "#### Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48592d2f",
   "metadata": {},
   "source": [
    "- és el model que ens farà el embdding. dos fases: \n",
    "- --- \n",
    "- constriur diccionari de paraules. \n",
    "- entrenar el model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5002b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4921f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec(min_count = 20, window = 2, alpha = 0.03, workers = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35edf46c",
   "metadata": {},
   "source": [
    "- vector_size: dimension of the word vectors\n",
    "- window: max. distance between related words within a sentence\n",
    "- min_count: ignore words with frequency lower than this\n",
    "- alpha: learning rate\n",
    "- max_vocab_size: unique words (10M words~ 1GB, prune infrequent words if not enough memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85de40d",
   "metadata": {},
   "source": [
    "- ---\n",
    "- n'hi ha més però els principals són: \n",
    "- vector_size: tamañ del convolutional head, dimensio del embeding. Deu tenir un valor per defecte.\n",
    "- window_max: donada una frase, un text, quantes paraules considerem, per davant o per darrere (paraules endavant o endarrere) considerem que poden tenir relació amb aquella. Definim un rang que considerem lògic. \n",
    "- hi ha el sample: que està relacionat amb el negative, que treiem els dos\n",
    "- min_count: les paraules que es repetiran molt i altres molt extrañes (aquestes són poc rellevants o no, perque son significatives d'aquell document). Si una apareix menys vegades que x li diguem en tot el nostre corpus, no la tindrem amb compte. \n",
    "- aqui el corpus es el dataset. \n",
    "- learning_rate: com apren. \n",
    "- a parte del min_count a més a més podem limitar el tamany màxim del vocabulari de 1000 paraules (max_vocab_size). A mida que n'hi ha que apareixen més sovint, anem eliminant les més frequents, el min count va pujant sol per quedar-nos amb el que volem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8a4ad",
   "metadata": {},
   "source": [
    "#### build vocabulary corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58dc318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.build_vocab(preprocessed_data, progress_per = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb604f38",
   "metadata": {},
   "source": [
    "- li passem les dades prosesades (tokens)\n",
    "- progress, si passen x coses et diu un missatge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e699b21",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cec42f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 152 ms, total: 22.5 s\n",
      "Wall time: 6.04 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(25791803, 38288880)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "w2v.train(preprocessed_data, total_examples = w2v.corpus_count, epochs = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69029ad3",
   "metadata": {},
   "source": [
    "#### embedded word vetor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "425ee809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7f6f71d14760>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1783327",
   "metadata": {},
   "source": [
    "- atribut, word vector de'n homer, bart (paraula homer, bart).\n",
    "- tenim una codificació de cada una de les paraules. \n",
    "- corelació entre paraules. \n",
    "- ubiquem paraules dins aquest espai. \n",
    "- 100 dimensions. \n",
    "- com cada una de les paraules de text han acabat codificades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cac15ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.24761464, -0.646954  , -0.22736843, -0.31750828, -0.9213416 ,\n",
       "        -0.13310805, -0.60613203,  0.56431097,  0.4907881 ,  0.7681977 ,\n",
       "         0.6250498 ,  0.6801206 ,  0.1126004 ,  0.5194235 , -0.25052863,\n",
       "         0.97861755,  0.36463523, -0.00327432, -0.903615  ,  0.25540316,\n",
       "         0.28269526, -0.2919843 ,  0.45271057,  0.27904296, -0.7701955 ,\n",
       "        -0.19838303,  0.34900033, -0.3017807 ,  0.46909657,  0.1214712 ,\n",
       "        -0.7149411 ,  0.3899995 , -0.625154  , -0.20385315, -0.10980447,\n",
       "        -0.40740964,  0.29905647,  0.9517245 , -0.03083142,  0.44666475,\n",
       "        -0.8710561 ,  1.128301  , -0.5629991 , -0.7861473 ,  0.3163792 ,\n",
       "        -0.7864798 , -0.04792315,  0.56814575,  0.36130708,  0.11740433,\n",
       "        -0.21058393,  0.08846226,  0.40662685, -1.5543088 ,  0.488568  ,\n",
       "        -0.0575441 , -0.11406124, -0.2408173 ,  1.4601722 ,  0.91371834,\n",
       "        -0.15842807, -0.99390143,  0.23752058, -0.8745389 , -0.10709973,\n",
       "        -0.00280077,  0.87932247,  0.3540226 ,  0.23275313, -1.0514233 ,\n",
       "        -0.01309839, -0.48554882, -1.2641944 ,  0.26822644,  0.6020975 ,\n",
       "         0.1230593 , -0.5880455 , -0.17560612,  0.0769409 ,  0.77040994,\n",
       "         0.79380333,  0.5700411 ,  0.15797065, -0.40109298, -0.25893798,\n",
       "         0.33879834,  1.1535661 ,  0.10860847, -0.47225538,  0.19844013,\n",
       "        -0.5286037 ,  0.2667765 ,  1.330669  ,  0.8577728 , -0.7985346 ,\n",
       "         0.48468456,  0.35082176,  0.69551104,  0.65126795, -0.2268575 ],\n",
       "       dtype=float32),\n",
       " array([ 0.7860693 , -0.3379231 ,  0.1406821 , -0.37673163, -0.57576233,\n",
       "        -0.42842552,  0.04031523,  0.19159777,  0.2928323 ,  0.7323577 ,\n",
       "        -0.04927982,  1.2067541 , -0.12623292, -0.28052258, -0.39547905,\n",
       "         0.9777901 ,  0.01768979,  0.7355812 , -0.6273902 , -0.19433886,\n",
       "         0.5646282 , -0.12156709,  0.1934748 ,  0.10425632, -0.52050245,\n",
       "        -0.22272658, -0.40858442, -0.17648026, -0.40420577,  0.02513201,\n",
       "         0.03508164, -0.49026397, -0.9116642 , -0.16868794, -0.33211297,\n",
       "        -0.8027729 ,  0.805043  ,  0.7861603 ,  0.7037357 ,  0.31536815,\n",
       "        -1.1780428 ,  1.3411351 , -0.86567813, -0.73828983, -0.07661003,\n",
       "        -0.95455575,  0.01538725,  0.3155793 , -0.2124014 , -0.02269271,\n",
       "        -0.4366145 , -0.06805152,  1.0091791 , -0.69446397,  1.0496727 ,\n",
       "        -0.43546548, -0.5465341 , -0.1682384 ,  1.2857562 ,  0.6910097 ,\n",
       "         0.09065639, -1.0702629 , -0.09966402, -0.9979247 , -0.5353247 ,\n",
       "         0.05612712,  0.5462673 ,  0.170435  , -0.42032167, -0.4755986 ,\n",
       "         0.29606476, -0.20741925, -0.47194344, -0.6007945 , -0.03822491,\n",
       "         0.09698161, -0.5940456 , -0.4621481 ,  0.68335015,  0.6665234 ,\n",
       "         0.6977879 ,  0.55377537,  0.18929207, -0.5639334 ,  0.13111201,\n",
       "         0.48567593,  0.62551457, -0.22003841, -1.0814333 , -0.10495115,\n",
       "        -0.48839542,  0.667017  ,  1.330624  ,  0.6410936 , -0.96534276,\n",
       "         0.21875162,  0.36135587, -0.17961314,  0.31387714, -0.20159261],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv['homer'], w2v.wv['bart']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e1f27a",
   "metadata": {},
   "source": [
    "#### word similarity (closeness in the embeddig space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f38fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7612679"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.similarity('homer', 'bart')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a781dbf3",
   "metadata": {},
   "source": [
    "- la similitud es el cosine similitarity. \n",
    "- com més petit el angle més properes són. \n",
    "- tots els numeros estàn entre 0 i 1, les diferencies no són molt grans. \n",
    "- esta tot en un hiper cub de tamany 1. \n",
    "- això són logits també. \n",
    "- això no es un convolutional head sinó un embeading head. \n",
    "- com que es el cos(x) va de -1 a 1. \n",
    "- com més angle, més semblants, vol dir que més juntes surten en el text. Cotxe i camió, son semblants i poden no sortir juntes, però és més probable que surtin juntes, si tenim prou mostres al final hauríem de sortir juntes. \n",
    "- perque això és una heurística. \n",
    "- podem tenir una mostre de tota la documentació escrita de la humanitat i no en tenim prou perque les inteligències generatives diguin tonteries. \n",
    "- pot ser una mostra minuscula respecte infinites possibilitats que tenim realment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce99e9",
   "metadata": {},
   "source": [
    "#### find most similar\n",
    "- és l'objectiu: trobar les més similars a una paraula. \n",
    "- amb una altre serè de preprosesament decideixen quina va darrere l'altre. \n",
    "    - sinó posariem sempre sinonims. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e885bf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('marge', 0.8120584487915039),\n",
       " ('bart', 0.7612677812576294),\n",
       " ('lisa', 0.6570351719856262),\n",
       " ('abe', 0.5900879502296448),\n",
       " ('grampa', 0.5653467178344727),\n",
       " ('mrs', 0.5507884621620178),\n",
       " ('homie', 0.5185571312904358),\n",
       " ('dad', 0.4895704686641693),\n",
       " ('family', 0.4702400863170624),\n",
       " ('mr', 0.45906394720077515)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive = ['homer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36419aa7",
   "metadata": {},
   "source": [
    "- similar: paraules que un cop fent l'embeding, aquelles paraules estàn allà aprop, això vol dir que aquestes paraules puguin apareixer conjuntament. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e2d0f",
   "metadata": {},
   "source": [
    "#### odd-one-out (find most dissimilar within list)\n",
    "- es dir que pases una llista de paraules i dius la que lliga menys. en termes de embeding, la que te menys cos similarity, la que està més lluny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b18c243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nelson'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.doesnt_match(['nelson', 'bart', 'milhouse'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79130a1",
   "metadata": {},
   "source": [
    "#### analogy difference\n",
    "\n",
    "- which word is to 'woman' as 'homer' is to 'marge' ?\n",
    "- relacionat amb: \n",
    "- quina paraula es a woman, lo que homer es a marge.\n",
    "- la qualitat de genere de bart és home, per tant la de lisa des dona, nena, etc\n",
    "- si asociem bart a home, asociem, lisa es una dona que encara es nena. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dfe68e80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 0.6092913150787354),\n",
       " ('child', 0.5805195569992065),\n",
       " ('bear', 0.5310059189796448)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive = [\"woman\", \"homer\"], negative = [\"marge\"], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340a96de",
   "metadata": {},
   "source": [
    "- which word is to 'woman' as 'bart' is to 'man'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "854ab736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lisa', 0.7314830422401428),\n",
       " ('homer', 0.5757341980934143),\n",
       " ('marge', 0.572046160697937)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.wv.most_similar(positive = [\"woman\", \"bart\"], negative = [\"man\"], topn = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a3099c",
   "metadata": {},
   "source": [
    "\n",
    "- el parametre negatiu\n",
    "\n",
    "- quan fem l'encoding (word embedding), els algoritmes es generen ells mateixos exemples negatius\n",
    "\n",
    "- positu: dues paraules seguides en el text que li ensenyes.\n",
    "- negatiu: de tot el corpus, de les dos paraules que van juntes, n'agafen una altre que no vagin juntes i li posen allà. això seria un exemple negatiu\n",
    "- conjunts de dos paraules: bi-grans o n-grams (n paraules juntes)\n",
    "\n",
    "- per cada exemple positiu, crea exemples negatius (és un parametre) són exemples que no estàn juntes. \n",
    "\n",
    "- la forma que tenim d'evaluar-ho és amb cross entrophi (span/ham). Mesura de loss per calcular l'error. \n",
    "\n",
    "- exemples positius: bi grams, grups de paraules que podem treure d'una frase que van una darrere l'altre\n",
    "- per cada un positiu, se'n inventa de negatius. \n",
    "\n",
    "- chatgpt 2 in 60 lines of numpy\n",
    "\n",
    "- https://jaykmody.com/blog/gpt-from-scratch/\n",
    "\n",
    "    - al chatgpt, hem d'entendre que el word embedding (no son paraules són tokens). token = unitat semantica\n",
    "    - una paraula pot tenir dos significat homes (home i plural) dos sentits semàntics (2 tokens). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
