{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d59b3ba",
   "metadata": {},
   "source": [
    "#### Proccès general:\n",
    "- Primer el que fem és un encoding del text. i tenim uns tokens_ids.\n",
    "    - De representació de text a representació manipulable per l'ordinador. \n",
    "    - Durant l'entrenament generarem un vocabulari amb el corpus que li hem passat.\n",
    "    - El vocabulari servirà per fer l'encoding. \n",
    "    - Entrenament, és construir el vocabulari i trobem els encodings. \n",
    "- Dos parts: Generar un encoding (fer els tokens).\n",
    "- Després necessitem un embeding per representar les relacions semantiques, gramaticals, etc de les paraules. \n",
    "- Encoding és convertir les paraules en numeros. Convertir en tokens i després amb numeros. \n",
    "- Hem de posar aquests numeros en un embedding.\n",
    "- ---\n",
    "- Teniem un input, el passem a tokens, els hi donem un id, construim el vocabulari amb token_id i fem un embeding per tenir vectors. \n",
    "- L'output és el vector de sortida de la xarxa neuronal de tots els tokens inicials com a input. \n",
    "- Després hem de fer el proccès invers per decodificar-lo. que serà un token_id, per trobar el string.\n",
    "- La reprojecció ens donarà el punt que està més a prop.\n",
    "- La reprojecció diu de totes les paraules que estàn en el vocabulari. quines estàn més aprop o tenen més similitud. \n",
    "- ---\n",
    "- Donat un input calcula la seguent paraula, el vector de la reprojecció dona el vector de la paraula optima, per saber-ho, mirem la similitud d'aquest punt per tots els altres. Anar al id del vocabulari que es fa servir per relacionar-ho amb les paraules.\n",
    "- El vocabulari són totes les paraules que hi ha en el corpus d'entrenament.\n",
    "- ---\n",
    "- Per generar les respostes de fins a 10 caracters i fa un bucle. \n",
    "- Li passem el prompt, calcula la paraula, el concatena amb el prompt i és el següent input del bucle. \n",
    "\n",
    "- parameters: com es la nn un cop entrenada. parametres són pesos.\n",
    "- hyperparametres: la arquitectura determina un numero de hyperparametres.\n",
    "\n",
    "#### hyperparametres\n",
    "- llargada del input\n",
    "- dimensions del embedding\n",
    "- n tokens sortida (output lenght)\n",
    "\n",
    "#### parametres del model\n",
    "- els parametres de la nn que fa l'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a2f22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 16:49:52.901461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6858de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a59e0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_encoder_hparams_and_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d86d5a",
   "metadata": {},
   "source": [
    "### load model: \n",
    "- ***encoder***\n",
    "- ***hyperparameters***\n",
    "- ***parameters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c097763",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_size = '124M'\n",
    "n_tokens_to_generate = 40\n",
    "encoder, hparams, params = load_encoder_hparams_and_params(model_size, 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294770ba",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad564d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8366e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = '''\n",
    "{\n",
    "  \"n_vocab\": 50257, # number of tokens in our vocabulary\n",
    "  \"n_ctx\": 1024, # maximum possible sequence length of the input (input length)\n",
    "  \"n_embd\": 768, # embedding dimension (determines the \"width\" of the network) (dimensions de l'espai, coordenades dels vectors, numero de neurones que tindrà el head)\n",
    "  \"n_head\": 12, # number of attention heads (n_embd must be divisible by n_head)\n",
    "  \"n_layer\": 12 # number of layers (determines the \"depth\" of the network)\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b3060b",
   "metadata": {},
   "source": [
    " ### Main loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb536a8",
   "metadata": {},
   "source": [
    "#### input encoding (tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0733cfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'what is your favorite musical band?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458b00e",
   "metadata": {},
   "source": [
    "- El encoder és el vocabulari més el tokenitzador. De paraula a token i de token a id.\n",
    "- Que sigui un numero és gracies al diccionari.\n",
    "- Agafa el input i mira quins tokens hi ha i va al vocabulari (diccionari) i pilla el id del token\n",
    "- pot ser que musical siguin dos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51393d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10919, 318, 534, 4004, 10530, 4097, 30]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = encoder.encode(prompt)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf5fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.decode(input_ids)\n",
    "# what is your favorite musical band?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb64763",
   "metadata": {},
   "source": [
    "#### output generation\n",
    "- rep input, parametres i nº tokens a generar.\n",
    "- fa looper nºtokens\n",
    "    - generem una sortida en base el input actual\n",
    "    - mirem la sortida, els logits,\n",
    "    - similitud de qualsevol token del diccionari amb la sortida optima\n",
    "    - greedy, ens quedem el logit més activat\n",
    "    - això és la id que hem de venir a buscar al diccionari vocabulari.\n",
    "    - la id la enganxem a els inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1984583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate(inputs, params, n_head, n_tokens_to_generate):\n",
    "\n",
    "    for _ in range(n_tokens_to_generate):\n",
    "        \n",
    "        logits = gpt2.gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = gpt2.np.argmax(logits[-1])                 # greedy sampling\n",
    "        inputs.append(int(next_id))                          # append prediction to input\n",
    "\n",
    "    return inputs # Sería la pregunta més la responsta perque els hem anant concatenant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d94422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10919, 318, 534, 4004, 10530, 4097, 30],\n",
       " [198, 198, 40, 1842, 262, 2647, 286, 262, 44249, 5542, 13])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens_to_generate = 11\n",
    "output_ids = _generate(input_ids, params, hparams['n_head'], n_tokens_to_generate)\n",
    "output_ids[:-n_tokens_to_generate], output_ids[-n_tokens_to_generate:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b05744b",
   "metadata": {},
   "source": [
    "#### output decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d3139ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is your favorite musical band?\n",
      "\n",
      "I love the music of the Grateful Dead.\n"
     ]
    }
   ],
   "source": [
    "print(encoder.decode(output_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b202af0",
   "metadata": {},
   "source": [
    "- 198 és l'intro. \n",
    "- 1 paraula no sempre es convertirà en un token, dependrà del entrenament. \n",
    "- li passem un text i genera una paraula i una altre i va fent. i respon al input que li hem donat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fdf29",
   "metadata": {},
   "source": [
    "### Autoregressive generation loop\n",
    "- Ho fem tot de cop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "245652c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_decode(inputs, params, n_head, n_tokens_to_generate):\n",
    "\n",
    "    print(inputs)\n",
    "    print(encoder.decode(inputs))\n",
    "    print()\n",
    "    \n",
    "    outputs = []\n",
    "    for _ in range(n_tokens_to_generate):\n",
    "        \n",
    "        logits = gpt2.gpt2(inputs, **params, n_head=n_head)  # model forward pass\n",
    "        next_id = gpt2.np.argmax(logits[-1])                 # greedy sampling\n",
    "        \n",
    "        inputs.append(int(next_id))                           # append prediction to input\n",
    "        outputs.append(int(next_id))                          # append prediction to output\n",
    "\n",
    "        # current input/output\n",
    "        print(inputs)\n",
    "        print(encoder.decode(outputs))\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf7f469f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10919, 318, 534, 4004, 10530, 4097, 30]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = encoder.encode(prompt)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b69d60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10919, 318, 534, 4004, 10530, 4097, 30, 198, 198, 40, 1842, 262, 2647, 286, 262, 44249, 5542, 13]\n",
      "what is your favorite musical band?\n",
      "\n",
      "I love the music of the Grateful Dead.\n",
      "\n",
      "[10919, 318, 534, 4004, 10530, 4097, 30, 198, 198, 40, 1842, 262, 2647, 286, 262, 44249, 5542, 13, 314]\n",
      " I\n"
     ]
    }
   ],
   "source": [
    "n_tokens_to_generate = 1\n",
    "output_ids = _generate_decode(input_ids, params, hparams['n_head'], n_tokens_to_generate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceb47ad",
   "metadata": {},
   "source": [
    "- Això és l'esquema bàsic.\n",
    "- Hi ha més coses com sortides de diferent llargada, diferents respostes en funció de la temperatura, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd78ba4",
   "metadata": {},
   "source": [
    "#### Multiheading\n",
    "- Vàries vegades les mateixes capes (en paral·lel) i dona un altre vector. \n",
    "- En podem tenir 10/20/100\n",
    "- Cada estructura, genera un embedding diferent.\n",
    "- Cada un detecta diferents matizos diferents de les paraules.\n",
    "- Multipliquem el numero de parametres que te el sistema, però el que fem és que quan generem una sortida, tenim més possibilitats.\n",
    "- Cada vector final (output) en tenim un per cada head.\n",
    "- Fem un entrenament per totes les nn i es van ajustant els pesos i al final cada head surt diferent.\n",
    "- Pot ser perque com que inicialitzem random, i anem ajustant, anem a minims locals diferents.\n",
    "- ---\n",
    "- un embedding que separa semanticament, un altre relacions sintactiques. Pretenem que la proximitat i similitud que expresen les distancies en els embeddings, capturi aspectes diferents de les relacions amb les paraules.\n",
    "- El mateix que feiem amb les xarxes convolucionals, que posavem diferents filtres (canals).\n",
    "- convinem tots els vectors finals, per reprojectar.\n",
    "- tots els embeddings tenen la mateixa dimensio.\n",
    "- Aquí es on hi ha l'aritmetica de rey - home = reina. Un embedding sería el que representa el genere.\n",
    "- ---\n",
    "- Un altre embedding que hi ha en aquest model i el separa és el word position embedding. Com s'ha generat, no ho sabem.\n",
    "    - no es el mateix una paraula al principi que al final. L'ordre importa.\n",
    "\n",
    "#### Característica dels models\n",
    "- Podem fer anar tot això sense model. \n",
    "- Es multiplicar els ids per una matriu que representa tot el que ha passat a la nn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
